# ğŸš€ ResNet50 ImageNet Training - Production Ready# ResNet50 ImageNet Training From Scratch - Complete Project ğŸš€



**Goal**: Train ResNet50 from scratch to achieve **81% top-1 accuracy** on ImageNet using AWS EC2 spot instances.**Goal**: Train ResNet50 from scratch on ImageNet 1K to achieve **81% Top-1 accuracy** - a challenging feat accomplished by only ~10,000 people worldwide!



**Strategy**: EMA (first 100 epochs) + SWA (last 20 epochs) with cosine annealing for optimal convergence.![Project Status](https://img.shields.io/badge/Status-Production%20Ready-brightgreen)

![Notebook Version](https://img.shields.io/badge/Notebook-v4-blue)

**Expected Cost**: ~$25 on g4dn.2xlarge spot instance![Target Accuracy](https://img.shields.io/badge/Target%20Accuracy-81%25-orange)



---## ğŸ¯ Project Overview



## ğŸ“‹ Quick StartThis is a **complete end-to-end project** for training ResNet50 from scratch (no pretrained weights) on ImageNet 1K using a **three-phase strategy**:



### 1. Setup EC2 Instance1. **ğŸ§ª Kaggle Validation Phase**: Test pipeline on TinyImageNet sample (200 classes)

```bash2. **ğŸ”§ EC2 Replication Phase**: Replicate setup on AWS EC2 with full environment

# Run the setup script3. **ğŸš€ Production Training Phase**: Full ImageNet 1K training with advanced techniques

chmod +x setup_ec2.sh

./setup_ec2.sh### Key Features

```- **Complete Production Pipeline**: Modular, scalable, and maintainable code

- **Advanced Techniques**: Mixup/CutMix, Label Smoothing, Mixed Precision Training

### 2. Prepare Dataset

#### ğŸš€ Fast ImageNet Download (NEW!)
**Use the optimized download system in `imagenet_download_optimized/` folder:**

```bash
cd imagenet_download_optimized

# For EC2 g4dn.2xlarge (Recommended)
export HF_TOKEN="your_hf_token_here"
chmod +x setup_and_download.sh
bash setup_and_download.sh
```

**Key Features:**
- âœ… **Smart Resume**: Exact point resume from interruptions
- âœ… **GPU Acceleration**: 3-5x faster (6-12 hours vs days)
- âœ… **Auto Setup**: Configures EC2 g4dn.2xlarge optimally
- âœ… **Error Recovery**: Handles spot instance interruptions

#### Manual ImageNet Setup
```bash
# Download ImageNet (or use existing)
mkdir -p /mnt/nvme_data/imagenet

# Place your ImageNet data in:
# /mnt/nvme_data/imagenet/train/  (1000 class folders)
# /mnt/nvme_data/imagenet/val/    (1000 class folders)
```

## ğŸ”„ Project Strategy & Implementation Plan

### Phase 1: Kaggle Validation ğŸ§ª
**Purpose**: Validate training pipeline and techniques on manageable dataset

```

- **Dataset**: TinyImageNet (200 classes, 100K images)

### 3. Start Training- **Environment**: Kaggle GPU (T4/P100, 16GB RAM)

```bash- **Duration**: 5 epochs (~30 minutes)

# Activate environment- **Batch Size**: 32 (memory-optimized for Kaggle)

conda activate pytorch_env- **Expected Accuracy**: 30-60% (proof of concept)



# Start training (runs in background with logging)**Key Validations**:

nohup python train.py \- âœ… Modular code structure works correctly

    --data /mnt/nvme_data/imagenet \- âœ… Advanced techniques (Mixup/CutMix) integrate properly

    --output-dir ./outputs \- âœ… Training loop handles mixed precision correctly

    --epochs 120 \- âœ… All artifacts generate successfully

    --batch-size 256 \

    > training.log 2>&1 &### Phase 2: EC2 Environment Replication ğŸ”§

**Purpose**: Replicate Kaggle environment on EC2 without consuming significant credits

# Monitor progress

tail -f training.log**Instance Configuration**: `g4dn.xlarge` (FREE TIER FRIENDLY)

```- **vCPUs**: 4 (Half of AWS limit, minimal cost)

- **Memory**: 16 GB (Sufficient for sample dataset)

### 4. Monitor Training- **GPU**: 1x NVIDIA T4 (16GB) - Same as Kaggle performance

```bash- **Storage**: 125 GB NVMe SSD (Fast I/O)

# Check current status- **Spot Price**: $0.113-0.151/hour (~75% savings)

python -c "

import torch**Dataset & Training**:

checkpoint = torch.load('./outputs/best_model.pth', map_location='cpu')- **Dataset**: ImageNet sample (100 classes, ~5K images)

print(f'Best Accuracy: {checkpoint[\"best_acc1\"]:.2f}%')- **Duration**: 2-4 hours (15-20 epochs)

print(f'Epoch: {checkpoint[\"epoch\"]}')- **Expected Accuracy**: 60-70% (same as Kaggle)

"- **Total Cost**: $0.50-$1.50 (minimal credit usage)



# View full logs**Key Validations**:

cat training.log | grep "Best:"- âœ… Identical environment to Kaggle setup

```- âœ… Quick validation without exhausting free credits

- âœ… Same GPU performance class (T4)

---- âœ… Environment setup scripts validated



## ğŸ¯ Expected Timeline & Milestones### Phase 3: Full ImageNet Production Training ğŸš€

**Purpose**: Achieve 81% top-1 accuracy on full ImageNet 1K with optimized cost-performance

| Epoch | Expected Accuracy | Time Elapsed | Strategy |

|-------|------------------|--------------|----------|**Instance Configuration**: `g4dn.2xlarge` (OPTIMAL BALANCE)

| 10    | ~30%            | ~8 hours     | EMA Warmup |- **vCPUs**: 8 (Exactly matches AWS limit)

| 30    | ~55%            | ~24 hours    | EMA Active |- **Memory**: 32 GB (Required for full ImageNet dataset)

| 50    | ~68%            | ~40 hours    | EMA Stable |- **GPU**: 1x NVIDIA T4 (16GB VRAM) - Sufficient for batch_size=64

| 81    | **>75%**        | ~65 hours    | **Milestone** |- **Storage**: 225 GB NVMe SSD (Fast I/O for 1.3M images)

| 90    | **>77%**        | ~72 hours    | **Milestone** |- **Spot Price**: $0.226-0.301/hour (~70% savings vs on-demand)

| 100   | ~79%            | ~80 hours    | EMAâ†’SWA Transition |

| 120   | **>81%**        | ~96 hours    | **ğŸ¯ TARGET** |**Training Configuration**:

- **Dataset**: Full ImageNet 1K (1000 classes, 1.3M training images)

---- **Duration**: 60-80 hours (100 epochs)

- **Batch Size**: 64 (optimized for T4 16GB VRAM)

## ğŸ’° Cost Tracking- **Advanced Techniques**: Mixup/CutMix, Label Smoothing, AMP

- **Total Cost**: $15-25 (realistic: ~$18.48)

### Instance Configuration- **Target**: 81% Top-1 validation accuracy

- **Instance**: g4dn.2xlarge

- **GPU**: NVIDIA T4 (16GB)**Cost-Performance Analysis**:

- **Spot Price**: ~$0.264/hour- **Balanced Choice**: g4dn.2xlarge offers optimal speed/cost ratio

- **Storage**: 500GB NVMe SSD- **Training Time**: 70 hours @ $0.264/hr = $18.48 total

- **Checkpointing**: Every 2 epochs for spot interruption recovery

### Cost Breakdown

```## ğŸ“ Project Structure

Training: 96 hours Ã— $0.264/hour = $25.34

Storage: 500GB Ã— $0.05/month â‰ˆ $0.80/week```

Total: ~$26 for complete trainingğŸ“¦ resnet50-imagenet-project/

```â”œâ”€â”€ ğŸ““ imagenet_kaggle_notebook_v4.ipynb    # Complete Kaggle pipeline

â”œâ”€â”€ ğŸ“‚ src/                                  # Modular source code

### Cost Monitoringâ”‚   â”œâ”€â”€ ğŸ§  model.py                         # ResNet50 implementation  

```bashâ”‚   â”œâ”€â”€ ğŸ¨ transforms.py                    # Data augmentation pipeline

# Check current costsâ”‚   â”œâ”€â”€ âš™ï¸  utils.py                        # Training utilities & config

aws ec2 describe-spot-price-history \â”‚   â”œâ”€â”€ ğŸƒ train.py                         # Main training framework

    --instance-types g4dn.2xlarge \â”‚   â”œâ”€â”€ ğŸ­ mixup.py                         # Advanced augmentation

    --product-descriptions "Linux/UNIX" \â”‚   â”œâ”€â”€ ğŸ” gradcam.py                       # Model interpretability

    --max-items 1â”‚   â””â”€â”€ ğŸ› debug_synthetic_run.py           # Testing utilities

```â”œâ”€â”€ ğŸ“‚ outputs/                             # Generated artifacts

â”‚   â”œâ”€â”€ ğŸ“„ training_log_v4.md              # Training progress logs

---â”‚   â”œâ”€â”€ ğŸ“Š training_history_v4.json        # Metrics data

â”‚   â”œâ”€â”€ ğŸ—ï¸  architecture_analysis_v4.md    # Model analysis

## ğŸ”§ Advanced Configurationâ”‚   â”œâ”€â”€ ğŸ¯ class_analysis_v4.md            # Per-class results

â”‚   â”œâ”€â”€ ğŸ” gradcam_summary_v4.md           # Visualization analysis

### Custom Training Parametersâ”‚   â”œâ”€â”€ ğŸ’¾ checkpoints/                    # Model checkpoints

```bashâ”‚   â””â”€â”€ ğŸ–¼ï¸  gradcam/                       # Visualization outputs

# High accuracy mode (slower but better)â”œâ”€â”€ ğŸ”§ setup_scripts/                       # EC2 setup automation

python train.py \â”‚   â”œâ”€â”€ ğŸ“œ setup_ec2.sh                    # Instance initialization

    --data /mnt/nvme_data/imagenet \â”‚   â”œâ”€â”€ ğŸ³ docker_setup.sh                 # Containerized environment

    --epochs 140 \â”‚   â””â”€â”€ ğŸ“‹ install_dependencies.sh         # Package installation

    --ema-epochs 110 \â”œâ”€â”€ ğŸ“Š monitoring/                          # Training monitoring

    --swa-epochs 30 \â”‚   â”œâ”€â”€ ğŸ“ˆ wandb_config.py                 # Weights & Biases setup

    --lr 0.08 \â”‚   â””â”€â”€ ğŸ“± tensorboard_setup.py            # TensorBoard configuration

    --mixup-prob 0.9â”œâ”€â”€ ğŸ“‹ requirements.txt                     # Python dependencies

â”œâ”€â”€ ğŸ”§ environment.yml                      # Conda environment

# Fast mode (90 epochs like friend's strategy)â””â”€â”€ ğŸ“– README.md                           # This documentation

python train.py \```

    --data /mnt/nvme_data/imagenet \

    --epochs 90 \## ğŸ—ï¸ Model Architecture - ResNet50 From Scratch

    --ema-epochs 80 \

    --swa-epochs 10 \### Core Specifications

    --lr 0.1| Component | Details |

```|-----------|---------|

| **Architecture** | ResNet50 with Bottleneck blocks |

### Resume Training| **Parameters** | 25.6M (25,557,032 trainable) |

```bash| **Model Size** | 97.5 MB |

# If training gets interrupted| **FLOPs** | 4.1 GFLOPs per forward pass |

python train.py \| **Receptive Field** | 267 pixels (119% input coverage) |

    --data /mnt/nvme_data/imagenet \| **Memory (Training)** | ~8GB for batch_size=64 |

    --resume ./outputs/best_model.pth \

    --output-dir ./outputs### Advanced Training Configuration

```

#### v4 Notebook Features

---- **ğŸ§ª Advanced Technique Testing**: Comprehensive validation of Mixup/CutMix

- **ğŸ”§ Bug-Free Implementation**: Fixed autocast deprecation and GradCAM issues  

## ğŸ“Š Results Analysis- **ğŸ“Š Rich Analysis**: Architecture tables, receptive field analysis, memory breakdown

- **ğŸ¯ Production Ready**: Modular imports, proper error handling, extensive logging

### Expected Final Results

```#### Anti-Overfitting Strategy

âœ… Target Achieved: 81.0%+ top-1 accuracy```python

ğŸ¯ Comparable to: ImageNet SOTA from-scratch trainingconfig = TrainingConfig()

ğŸ’¡ Key Techniques: EMA + SWA + Cosine Annealing + Mixup# Weight Decay: 3e-4 (L2 regularization)

â±ï¸  Training Time: ~96 hours (~4 days)# Label Smoothing: 0.15 (better generalization)  

ğŸ’° Total Cost: ~$26# Mixup Alpha: 0.2 (data augmentation)

```# CutMix Alpha: 1.0 (spatial augmentation)

# Warmup Epochs: 5 (stable training start)

### Model Performance# Cosine LR Schedule: Smooth convergence

```bash```

# Test final model

python -c "## ğŸ“Š Model Architecture & Analysis

import torch

from src.model import get_model### Model Summary

from src.utils import accuracy| Component | Details |

|-----------|---------|

# Load best model| **Architecture** | ResNet50 with Bottleneck blocks |

model = get_model('resnet50', num_classes=1000)| **Total Parameters** | 25,557,032 |

checkpoint = torch.load('./outputs/best_model.pth')| **Trainable Parameters** | 25,557,032 |

model.load_state_dict(checkpoint['model_state_dict'])| **Model Size** | 97.5 MB |

| **Input Size** | 224Ã—224Ã—3 |

print(f'Model Type: {checkpoint[\"model_type\"]}')| **Output Classes** | 1000 (ImageNet) |

print(f'Best Accuracy: {checkpoint[\"best_acc1\"]:.2f}%')| **Approximate FLOPs** | 4.1 GFLOPs |

print(f'Training Epoch: {checkpoint[\"epoch\"]}')

"### Layer-wise Parameter Distribution

```| Layer Type | Parameters | Percentage |

|------------|------------|-----------|

---| **Final Classifier (fc)** | 2,049,000 | 8.0% |

| **Layer 4 Bottlenecks** | 14,942,720 | 58.4% |

## ğŸš¨ Troubleshooting| **Layer 3 Bottlenecks** | 6,039,552 | 23.6% |

| **Layer 2 Bottlenecks** | 1,512,448 | 5.9% |

### Common Issues| **Layer 1 Bottlenecks** | 379,392 | 1.5% |

| **Initial Conv + BN** | 9,472 | 0.04% |

#### 1. Out of Memory

```bash### Receptive Field Analysis

# Reduce batch size| Layer | Kernel | Stride | Receptive Field | Output Size | Jump |

python train.py --batch-size 128  # Instead of 256|-------|--------|--------|-----------------|-------------|------|

```| Input | - | - | 1 | 224Ã—224 | 1 |

| conv1 | 7Ã—7 | 2 | 7 | 112Ã—112 | 2 |

#### 2. Slow Data Loading| maxpool | 3Ã—3 | 2 | 11 | 56Ã—56 | 4 |

```bash| layer1 | 3Ã—3 | 1 | 19 | 56Ã—56 | 4 |

# Increase workers| layer2 | 3Ã—3 | 2 | 27 | 28Ã—28 | 8 |

python train.py --workers 16  # Instead of 8| layer3 | 3Ã—3 | 2 | 43 | 14Ã—14 | 16 |

```| layer4 | 3Ã—3 | 2 | 75 | 7Ã—7 | 32 |

| avgpool | 7Ã—7 | 7 | 267 | 1Ã—1 | 224 |

#### 3. Spot Instance Interruption

```bash**Key Insights:**

# Check interruption warnings- ğŸ¯ **Final Receptive Field**: 267 pixels (119% of input image)

curl -s http://169.254.169.254/latest/meta-data/spot/instance-action- âœ… **Full Coverage**: Receptive field covers entire 224Ã—224 input

- ğŸ”„ **Total Downsampling**: 32Ã— (224â†’7 feature maps)

# Auto-resume script- ğŸ“Š **Feature Density**: 7Ã—7Ã—2048 = 100,352 features before classification

#!/bin/bash

while true; do### Architecture Design Choices

    if [ -f "./outputs/best_model.pth" ]; then

        python train.py --resume ./outputs/best_model.pth --data /mnt/nvme_data/imagenet**ImageNet-Specific Optimizations:**

    else- **7Ã—7 Initial Conv**: Larger receptive field for high-resolution inputs

        python train.py --data /mnt/nvme_data/imagenet- **Stride-2 + MaxPool**: Aggressive early downsampling to manage computation

    fi- **Bottleneck Blocks**: 1Ã—1â†’3Ã—3â†’1Ã—1 design reduces parameters while maintaining capacity

    sleep 60- **Batch Normalization**: After every convolution for stable training

done- **Global Average Pooling**: Replaces fully connected layers, reduces overfitting

```

**Training-from-Scratch Considerations:**

#### 4. Low Accuracy- **He Initialization**: Kaiming normal for ReLU networks

```bash- **Zero-init Residual**: Last BN in each block initialized to zero

# Check if milestones are met:- **No Dropout**: ResNet50 typically doesn't use dropout (relies on residual connections)

# Epoch 81: Should be >75%- **Deep Architecture**: 50 layers provide sufficient capacity for ImageNet complexity

# Epoch 90: Should be >77%

# If not, verify dataset and try higher learning rate**Memory & Computation:**

```- **Peak Memory**: ~8GB for batch_size=64 with mixed precision

- **Training Speed**: ~4.1 GFLOPs per forward pass

---- **Gradient Memory**: ~2Ã— model size during backpropagation



## ğŸ“ Output Files## ğŸ§ª Quick Start: Kaggle Testing (Phase 1)



After training, you'll have:### 1. Setup Kaggle Environment

```1. **Create Kaggle Account**: Sign up at [kaggle.com](https://kaggle.com)

outputs/2. **Enable GPU**: Settings â†’ Accelerator â†’ GPU T4 x2

â”œâ”€â”€ best_model.pth          # Best model weights3. **Upload Notebook**: Import `imagenet_kaggle_notebook_v4.ipynb`

â”œâ”€â”€ training.log            # Detailed logs4. **Enable Internet**: For package installations

â””â”€â”€ checkpoints/            # Periodic saves

```### 2. Expected Kaggle Results

```

---ğŸ–¥ï¸  Device Status: CUDA (Tesla T4) - Mixed Precision ENABLED âš¡



## âœ… Success CriteriağŸ“Š v4 Training Results (5 epochs on TinyImageNet):

   â€¢ Dataset: 200 classes, 100K training images

**Training is successful if**:   â€¢ Batch Size: 32 (Kaggle optimized)

- [x] Reaches >75% accuracy by epoch 81   â€¢ Training Time: ~30 minutes

- [x] Reaches >77% accuracy by epoch 90   â€¢ Batches per Epoch: 3,125

- [x] Achieves >81% final accuracy   â€¢ Final Training Accuracy: 45-65%

- [x] Completes within budget (~$30)   â€¢ Final Validation Accuracy: 35-55%

- [x] No major interruptions or errors

ğŸ“ Generated Artifacts:

**If accuracy is below target**:   âœ… training_log_v4.md - Complete epoch logs

1. Check data loading (ImageNet format)   âœ… architecture_analysis_v4.md - Model structure

2. Verify GPU utilization (should be >90%)   âœ… gradcam/ - 6 visualization samples  

3. Check learning rate schedule   âœ… confusion_matrix_v4.png - Class analysis

4. Ensure EMAâ†’SWA transition is working   âœ… resnet50_v4_final.pth - Model checkpoint

```

---

### 3. Key v4 Improvements

## ğŸ‰ Next Steps After Success- **ï¿½ No Deprecation Warnings**: Fixed PyTorch autocast issues

- **ğŸ“Š Enhanced Monitoring**: Clear CUDA/CPU detection and status

1. **Save Results**: Download model and logs- **ğŸ­ Advanced Augmentation**: Properly integrated Mixup/CutMix

2. **Document**: Record exact accuracy and cost- **ğŸ” Rich Visualizations**: GradCAM working with correct API

3. **Optimize**: Try different hyperparameters for >82%- **ğŸ“ˆ Better Progress Tracking**: tqdm bars with meaningful metrics

4. **Deploy**: Use model for inference or transfer learning

## ğŸš€ EC2 Production Setup (Phase 2 & 3)

---

### Instance Requirements

**ğŸš€ Happy Training! Target: 81% accuracy for ~$25**

| Phase | Instance Type | GPUs | vCPUs | RAM | Storage | Spot Price* | Use Case |

---|-------|---------------|------|-------|-----|---------|-------------|----------|

| **Phase 2** (Testing) | `g4dn.xlarge` | 1x T4 | 4 | 16 GB | 125GB NVMe | $0.113-0.151 | Environment replication |

## ğŸ“ Project Structure| **Phase 3** (Production) | `g4dn.2xlarge` | 1x T4 | 8 | 32 GB | 225GB NVMe | $0.226-0.301 | Full ImageNet training |



```*Spot instance pricing with ~70-75% savings vs on-demand

s9_assignment/

â”œâ”€â”€ train.py                # Main training script### EC2 Setup Process

â”œâ”€â”€ requirements.txt        # Python dependencies

â”œâ”€â”€ setup_ec2.sh           # EC2 setup automation#### 1. Launch Instance

â”œâ”€â”€ README.md              # This file```bash

â”œâ”€â”€ src/                   # Core modules# Use Deep Learning AMI (Ubuntu 18.04/20.04)

â”‚   â”œâ”€â”€ model.py           # ResNet50 implementation# AMI ID: ami-0c6b1d09930fac512 (check latest)

â”‚   â”œâ”€â”€ transforms.py      # Data augmentationaws ec2 run-instances \

â”‚   â”œâ”€â”€ mixup.py           # Mixup/CutMix  --image-id ami-0c6b1d09930fac512 \

â”‚   â”œâ”€â”€ ema.py             # EMA implementation  --instance-type p3.8xlarge \

â”‚   â”œâ”€â”€ utils.py           # Training utilities  --key-name your-key-pair \

â”‚   â””â”€â”€ gradcam.py         # Visualization  --security-groups deep-learning-sg \

â””â”€â”€ imagenet_real_sample/  # Sample dataset  --block-device-mappings '[{"DeviceName":"/dev/sda1","Ebs":{"VolumeSize":1000,"VolumeType":"gp3"}}]'

    â”œâ”€â”€ train/```

    â””â”€â”€ val/

```#### 2. Connect and Setup
```bash
# Connect to instance
ssh -i your-key.pem ubuntu@ec2-xx-xxx-xxx-xxx.compute-1.amazonaws.com

# Clone project
git clone https://github.com/yourusername/resnet50-imagenet-project.git
cd resnet50-imagenet-project

# Setup environment
bash setup_scripts/setup_ec2.sh
```

#### 3. Download ImageNet Dataset
```bash
# Option 1: Pre-downloaded (recommended)
aws s3 sync s3://your-imagenet-bucket/ILSVRC2012 ./data/imagenet/

# Option 2: Direct download (requires ImageNet account)
# Register at image-net.org first
wget [ImageNet-URL] -O imagenet.tar
bash setup_scripts/extract_imagenet.sh imagenet.tar
```

#### 4. Validate Environment (Phase 2)
```bash
# Launch g4dn.xlarge spot instance
aws ec2 request-spot-instances \
  --spot-price "0.20" \
  --instance-count 1 \
  --type "one-time" \
  --launch-specification '{
    "ImageId": "ami-0c02fb55956c7d316",
    "InstanceType": "g4dn.xlarge",
    "KeyName": "your-key-pair-name",
    "SecurityGroupIds": ["sg-your-security-group"]
  }'

# Connect and setup environment
ssh -i your-key.pem ubuntu@instance-ip
sudo apt update && sudo apt install -y nvidia-driver-470 python3-pip git
pip3 install torch torchvision torchaudio tqdm numpy matplotlib Pillow

# Download ImageNet sample (100 classes, replicating Kaggle)
wget https://github.com/fastai/imagenette/releases/download/v2/imagenette2-320.tgz
tar -xzf imagenette2-320.tgz && mv imagenette2-320 ~/imagenet_sample

# Test training (15 epochs, same as Kaggle)
python -m src.train \
  --data ~/imagenet_sample \
  --epochs 15 \
  --batch-size 32 \
  --lr 0.1 \
  --workers 4 \
  --amp \
  --output-dir ./test_outputs

# Expected: 60-70% accuracy in 2-4 hours, cost: $0.50-$1.50
```

#### 5. Full Training (Phase 3)
```bash
# Launch g4dn.2xlarge production instance
aws ec2 request-spot-instances \
  --spot-price "0.35" \
  --instance-count 1 \
  --type "one-time" \
  --launch-specification '{
    "ImageId": "ami-0c02fb55956c7d316", 
    "InstanceType": "g4dn.2xlarge",
    "KeyName": "your-key-pair-name",
    "BlockDeviceMappings": [{
      "DeviceName": "/dev/xvda",
      "Ebs": {"VolumeSize": 500, "VolumeType": "gp3"}
    }]
  }'

# Setup production environment and download full ImageNet
mkdir -p /mnt/nvme_data/imagenet/{train,val}

# Extract and organize training data
tar -xf ILSVRC2012_img_train.tar -C train/
cd train && for f in *.tar; do mkdir -p "${f%.tar}" && tar -xf "$f" -C "${f%.tar}" && rm "$f"; done

# Production training with optimized parameters
python -m src.train \
  --data /mnt/nvme_data/imagenet \
  --epochs 100 \
  --batch-size 64 \
  --lr 0.1 \
  --weight-decay 2e-4 \
  --momentum 0.9 \
  --warmup-epochs 5 \
  --label-smoothing 0.1 \
  --mixup-alpha 0.4 \
  --cutmix-alpha 1.0 \
  --workers 8 \
  --amp \
  --output-dir ./outputs \
  --checkpoint-freq 2 \
  --save-best-only \
  --resume-latest

# Expected: 60-80 hours training, ~$18.48 total cost, 81% target accuracy
```

## ğŸ† Model Comparison & Benchmarks

### ResNet Family Comparison
| Model | Parameters | FLOPs | Top-1 Acc* | Top-5 Acc* | Our Target |
|-------|------------|-------|-------------|-------------|------------|
| **ResNet50** | **25.6M** | **4.1G** | **76.1%** | **92.9%** | **ğŸ¯ 81.0%** |
| ResNet34 | 21.8M | 3.7G | 73.3% | 91.4% | - |
| ResNet101 | 44.5M | 7.8G | 77.4% | 93.5% | - |
| ResNet152 | 60.2M | 11.6G | 78.3% | 94.1% | - |

*Standard ImageNet results with proper training

### Training Efficiency Analysis
| Metric | Value | Comparison |
|--------|-------|------------|
| **Parameters vs Accuracy** | 25.6M â†’ 81% | Excellent efficiency |
| **FLOPs vs Accuracy** | 4.1G â†’ 81% | Optimal for deployment |
| **Training Time** | ~12-48 hours | Reasonable on modern GPUs |
| **Memory Usage** | ~8GB (bs=64) | Fits on most modern GPUs |
| **Convergence Speed** | ~100 epochs | Standard for from-scratch |

## ğŸ“Š Training Hyperparameters for 81% Accuracy

| Parameter | Value | Notes |
|-----------|-------|-------|
| **Epochs** | 100-200 | Start with 100, extend if needed |
| **Batch Size** | 256 (per GPU) | Scale with available GPUs |
| **Learning Rate** | 0.5 | For batch size 256; scale linearly |
| **LR Schedule** | Cosine + Warmup | 5-10 epoch warmup, then cosine decay |
| **Optimizer** | SGD + Nesterov | momentum=0.9, weight_decay=1e-4 |
| **Augmentation** | Strong | ColorJitter, RandomErasing, eventually Mixup |
| **Label Smoothing** | 0.1 | Regularization for from-scratch training |
| **Mixed Precision** | âœ… Enabled | Faster training, lower memory |

### Advanced Techniques for 81%
- **Mixup/CutMix**: Label and image mixing augmentation
- **EMA**: Exponential moving average of model weights  
- **AutoAugment**: Learned augmentation policies
- **Stochastic Depth**: Randomly skip residual blocks during training
- **Multi-Scale Training**: Vary input resolution during training

## ï¿½ Expected Training Progression

### Kaggle Phase (TinyImageNet, 5 epochs)
| Epoch | Train Loss | Train Acc | Val Acc | Time | Notes |
|-------|------------|-----------|---------|------|-------|
| 1 | 5.2 | 8% | 6% | 6min | Initial learning |
| 2 | 3.8 | 22% | 18% | 6min | Rapid improvement |  
| 3 | 2.9 | 35% | 28% | 6min | Steady progress |
| 4 | 2.3 | 45% | 38% | 6min | Convergence starts |
| 5 | 1.9 | 55% | 45% | 6min | Good generalization |

### EC2 Phase 2 (TinyImageNet, 20 epochs)
- **Faster Training**: ~3min/epoch (vs 6min on Kaggle)
- **Better Accuracy**: 60-70% validation accuracy  
- **Resource Utilization**: Full V100 utilization

### EC2 Phase 3 (Full ImageNet, 100 epochs)
| Epoch Range | Expected Top-1 Acc | Timeline | Key Milestones |
|-------------|---------------------|----------|----------------|
| 1-10 | 5-25% | Hours 0-2 | Warmup, basic features |
| 11-30 | 25-50% | Hours 2-8 | Object recognition |
| 31-60 | 50-70% | Hours 8-16 | Fine-grained features |
| 61-90 | 70-78% | Hours 16-30 | ResNet50 baseline |
| 91-100+ | 78-81% | Hours 30-48 | Advanced techniques |

## ğŸ¯ Advanced Techniques for 81% Target

### Implemented in v4
- âœ… **Mixup/CutMix**: Advanced data augmentation
- âœ… **Label Smoothing**: Improved generalization  
- âœ… **Mixed Precision**: Faster training, lower memory
- âœ… **Warmup + Cosine LR**: Optimal learning rate schedule
- âœ… **Weight Decay**: L2 regularization

### For 81% Breakthrough
- ğŸ”„ **EMA (Exponential Moving Average)**: Model weight averaging
- ğŸ”„ **AutoAugment**: Learned augmentation policies
- ğŸ”„ **Stochastic Depth**: Random layer skipping
- ğŸ”„ **Multi-Scale Training**: Variable input resolution
- ğŸ”„ **Extended Training**: 150-200 epochs

## ğŸ† Project Milestones & Deliverables

### Phase 1 Completed âœ…
- [x] **v4 Notebook**: Production-ready Kaggle pipeline
- [x] **Modular Architecture**: Clean `src/` module structure
- [x] **Advanced Techniques**: Mixup/CutMix integration
- [x] **Comprehensive Analysis**: Architecture, GradCAM, confusion matrix
- [x] **Bug Fixes**: Autocast deprecation, GradCAM API issues

### Phase 2 Targets ğŸ¯
- [ ] **EC2 Environment**: Replicated Kaggle setup on AWS
- [ ] **Environment Scripts**: Automated setup and configuration
- [ ] **Performance Validation**: Faster training, identical results
- [ ] **Resource Monitoring**: GPU utilization, memory usage tracking

### Phase 3 Targets ğŸš€  
- [ ] **Full Dataset Training**: Complete ImageNet 1K pipeline
- [ ] **81% Accuracy**: Target validation performance
- [ ] **Model Artifacts**: Final checkpoints and analysis
- [ ] **Documentation**: Complete training logs and insights

### Deployment Targets ğŸŒ
- [ ] **HuggingFace Space**: Live inference application
- [ ] **Model Hub**: Published trained model
- [ ] **GitHub Repository**: Complete open-source project
- [ ] **Technical Blog**: Project walkthrough and insights

## ğŸ’° Cost Estimation

### Kaggle Phase (Free)
- **Cost**: $0 (Kaggle free GPU hours)
- **Time**: 30 minutes
- **Usage**: 0.5 GPU hours

### EC2 Phase 2 (Testing)
- **Instance**: g4dn.xlarge @ $0.113-0.151/hour (spot)
- **Duration**: 2-4 hours (environment replication)
- **Cost**: $0.50-$1.50 (minimal credit usage)

### EC2 Phase 3 (Production)  
- **Instance**: g4dn.2xlarge @ $0.226-0.301/hour (spot)
- **Duration**: 60-80 hours (full ImageNet training)
- **Realistic Cost**: 70 hours Ã— $0.264/hour = $18.48
- **Storage & Transfer**: ~$3

**Total Estimated Cost**: $22-25 for complete project (97% savings vs original estimate)

## ğŸ—ï¸ Local Development Setup

```bash
# 1. Clone repository
git clone https://github.com/your-username/resnet50-imagenet-scratch.git
cd resnet50-imagenet-scratch

# 2. Create environment
conda create -n resnet50 python=3.10
conda activate resnet50

# 3. Install dependencies
pip install -r requirements.txt

# 4. Test synthetic run (no data needed)
python -m src.debug_synthetic_run

# 5. Test with sample data
python -m src.train \
  --data /path/to/imagenet/sample \
  --epochs 2 \
  --batch-size 32 \
  --output-dir ./test_outputs
```

## ğŸ”§ Technical Implementation Details

### Model Initialization
- **Conv2D**: He (Kaiming) normal initialization for ReLU networks
- **BatchNorm2D**: weight=1, bias=0 (standard)
- **Linear**: Normal distribution, std=0.01

### Data Pipeline
- **Training**: RandomResizedCrop, RandomHorizontalFlip, ColorJitter, RandomErasing
- **Validation**: Resize â†’ CenterCrop â†’ Normalize
- **Normalization**: ImageNet statistics (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])

### Training Loop
- **Mixed Precision**: Automatic Mixed Precision (AMP) for speed
- **Gradient Scaling**: Handle mixed precision gradients correctly
- **Checkpointing**: Save best model + latest model every N epochs
- **Logging**: Both console output and markdown file

## ğŸš¨ Common Issues & Solutions

### Memory Issues
```bash
# Reduce batch size
--batch-size 128  # Instead of 256

# Reduce workers
--workers 4       # Instead of 16

# Enable gradient checkpointing (if implemented)
--gradient-checkpointing
```

### Slow Training
```bash
# Enable mixed precision
--amp

# Increase batch size (if memory allows)
--batch-size 512

# More workers (if CPU allows)
--workers 32
```

### Poor Convergence
```bash
# Longer warmup
--warmup-epochs 10

# Lower learning rate
--lr 0.1

# More regularization
--label-smoothing 0.2
```

## ğŸ¯ Performance Targets

| Metric | Kaggle Demo | EC2 Full Training |
|--------|-------------|-------------------|
| **Runtime** | 5-30 minutes | 12-48 hours |
| **GPU Memory** | 4-6 GB | 15+ GB |
| **Top-1 Accuracy** | 30-60% (subset) | 81% (full ImageNet) |
| **Dataset Size** | 1K-10K images | 1.2M images |

## ğŸ“ Support & Resources

- **ImageNet Access**: [image-net.org](http://image-net.org) registration required
- **Papers**: [Deep Residual Learning](https://arxiv.org/abs/1512.03385), [Bag of Tricks](https://arxiv.org/abs/1812.01187)
- **References**: [DAWNBench](https://dawn.cs.stanford.edu/benchmark/), [Papers With Code](https://paperswithcode.com/sota/image-classification-on-imagenet)

## ğŸ“œ License

MIT License - Feel free to use for educational purposes.

## ğŸ‰ Achievement Unlock

### Current Status: v4 Production Pipeline Ready âœ…
- Complete modular implementation
- Advanced techniques integrated  
- Comprehensive analysis and monitoring
- Bug-free, warning-free training
- Ready for EC2 scaling

### Next Milestone: 81% Accuracy ğŸ¯
Upon reaching 81% top-1 accuracy on ImageNet 1K, you'll join an exclusive group of approximately **10,000 people worldwide** who have successfully trained ImageNet from scratch!

## ğŸ“œ License

MIT License - Feel free to use for educational and research purposes.

---

**Ready to train ResNet50 from scratch and join the 81% club?** ğŸš€

Start with Phase 1 on Kaggle, then scale to EC2 for the full challenge!
